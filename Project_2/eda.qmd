---
title: "preprocessing"
format: md
editor: visual
---

## Preprocessing

```{python}
import nltk
nltk.download('vader_lexicon')
```

```{python}
detectors.head(2)
```

```{python}
detectors.shape
```

```{python}
import os # checking existence, seeing if the file is in the right place
print(os.path.exists('detectors_prepped_with_document.csv'))
```

```{python}
df = pd.read_csv('/Users/ruchadesh/Documents/Documents - Rucha’s MacBook Air/GitHub/STAT-155/Project 2/detectors_prepped_with_document.csv') # load in the dataset

sia = SentimentIntensityAnalyzer() # initialize the SIA analyzer
```

```{python}
keywords = ["I", "my", "we", "me", 
            "feel", "love", "hate",
            "afraid", "gonna", "wanna",
            "you", "know", "kinda", "like", "uh"]
# define the keyword list
```

```{python}
def word_count(text):
    return len(word_tokenize(text))

def char_count(text):
    return len(text)

def sentence_count(text):
    return len(sent_tokenize(text))

def avg_word_length(text):
    words = word_tokenize(text)
    return np.mean([len(word) for word in words]) if words else 0

def unique_word_count(text):
    return len(set(word_tokenize(text)))

def readability_score(text):
    try:
        return textstat.flesch_reading_ease(text)
    except:
        return None

def sentiment_score(text):
    return sia.polarity_scores(text)["compound"]

def keyword_presence(text, keywords):
    text_lower = text.lower()
    return {f"keyword_{kw}": int(kw.lower() in text_lower) for kw in keywords}

def pos_counts(text):
    tags = pos_tag(word_tokenize(text))
    counts = Counter(tag for word, tag in tags)
    return {
        "noun_count": sum(counts[tag] for tag in counts if tag.startswith("NN")),
        "verb_count": sum(counts[tag] for tag in counts if tag.startswith("VB")),
        "adj_count": sum(counts[tag] for tag in counts if tag.startswith("JJ")),
        "adv_count": sum(counts[tag] for tag in counts if tag.startswith("RB")),
```

```{python}

from nltk import pos_tag, word_tokenize
from collections import Counter

df["word_count"] = df["document"].apply(word_count)
df["char_count"] = df["document"].apply(char_count)
df["sentence_count"] = df["document"].apply(sentence_count)
df["avg_word_length"] = df["document"].apply(avg_word_length)
df["unique_word_count"] = df["document"].apply(unique_word_count)
df["readability_score"] = df["document"].apply(readability_score)
df["sentiment_score"] = df["document"].apply(sentiment_score)

# Expand keyword presence
keyword_df = df["document"].apply(lambda x: pd.Series(keyword_presence(x, keywords)))
df = pd.concat([df, keyword_df], axis=1)

# Expand part of speech counts
pos_df = df["document"].apply(lambda x: pd.Series(pos_counts(x)))
df = pd.concat([df, pos_df], axis=1)

# Save result
df.to_csv("detectors_enriched.csv", index=False)
print("Saved enriched dataset as 'detectors_enriched.csv'")
```

```{python}
df.head(2).T
```

```{python}
df.columns
```

```{python}
df.head().T
```

```{python}
df2 = pd.read_csv('/Users/ruchadesh/Documents/Documents - Rucha’s MacBook Air/GitHub/STAT-155/Project 2/detectors_enriched.csv')

df2.shape
```

```{python}
print(df2.info())
```

```{python}
print(df2.describe())
```

```{python}
print(df2.head())
```

## EDA

```{python}
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
```

```{python}
# Load the dataset
df2 = pd.read_csv("detectors_enriched.csv")

# All plots created have a white background
sns.set(style="whitegrid")
```

```{python}
# Basic data overview (data type, count, null count) and summary statistics (mean, median, mode, etc)
print("Dataset Overview:")
print(df2.info())
print("Summary Statistics:")
print(df2.describe(include='all'))
```

```{python}
# Bar chart for class balance, predicted human vs predicted ai
plt.figure(figsize=(6, 4))
sns.countplot(data=df2, x=".pred_class", palette="Set2")
plt.title("Class Distribution (Predicted)")
plt.xlabel("Predicted Class")
plt.ylabel("Count")
plt.tight_layout()
plt.show()
```

```{python}
# Written by native vs. non-native english speaker
plt.figure(figsize=(6, 4))
sns.countplot(data=df2, x="native", hue=".pred_class", palette="pastel")
plt.title("Predicted Class by Native Status")
plt.xlabel("Native English Writer")
plt.ylabel("Count")
plt.tight_layout()
plt.show()
```

```{python}
# type of prompt and what it was flagged as
plt.figure(figsize=(10, 4))
sns.countplot(data=df2, x="prompt", hue=".pred_class", palette="muted")
plt.title("Predicted Class by Prompt Style")
plt.xlabel("Prompt Type")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```

```{python}
# Distribution of the readability score (FLESCH)
plt.figure(figsize=(8, 4))
sns.histplot(df2["readability_score"], kde=True, bins=30, color="skyblue")
plt.title("Readability Score Distribution")
plt.xlabel("Flesch Reading Ease Score")
plt.tight_layout()
plt.show()
```

```{python}
# Boxplot of Sentiment score 
plt.figure(figsize=(6, 4))
sns.boxplot(data=df2, x=".pred_class", y="sentiment_score", palette="Set3")
plt.title("Sentiment Score by Predicted Class")
plt.tight_layout()
plt.show()
```

```{python}
# Correlation matrix
numeric_cols = df2.select_dtypes(include=np.number)
plt.figure(figsize=(12, 10))
sns.heatmap(numeric_cols.corr(), annot=True, fmt=".2f", cmap="coolwarm", center=0)
plt.title("Correlation Matrix")
plt.tight_layout()
plt.show()
```

```{python}
# Word count vs readability
plt.figure(figsize=(6, 4))
sns.scatterplot(data=df2, x="word_count", y="readability_score", hue=".pred_class", alpha=0.6)
plt.title("Word Count vs. Readability")
plt.tight_layout()
plt.show()
```

```{python}
# average word length by class
plt.figure(figsize=(6, 4))
sns.boxplot(data=df2, x=".pred_class", y="avg_word_length", palette="Set2")
plt.title("Average Word Length by Predicted Class")
plt.tight_layout()
plt.show()
```
